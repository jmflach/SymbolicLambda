Index: src/evaluator.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Copyright (c) 2020-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom logging import getLogger\nfrom collections import OrderedDict\nfrom concurrent.futures import ProcessPoolExecutor\nimport os\nimport torch\nimport sympy as sp\n\nfrom .utils import to_cuda, timeout, TimeoutError\nfrom .envs.char_sp import InvalidPrefixExpression, is_valid_expr\nfrom .envs.sympy_utils import simplify\n\n\nlogger = getLogger()\n\n\nBUCKET_LENGTH_SIZE = 5\n\n\ndef idx_to_sp(env, idx, return_infix=False):\n    \"\"\"\n    Convert an indexed prefix expression to SymPy.\n    \"\"\"\n    prefix = [env.id2word[wid] for wid in idx]\n    prefix = env.unclean_prefix(prefix)\n    infix = env.prefix_to_infix(prefix)\n    eq = sp.S(infix, locals=env.local_dict)\n    return (eq, infix) if return_infix else eq\n\n\n@timeout(5)\ndef check_valid_solution(env, src, tgt, hyp):\n    \"\"\"\n    Check that a solution is valid.\n    \"\"\"\n    f = env.local_dict['f']\n    x = env.local_dict['x']\n\n    valid = simplify(hyp - tgt, seconds=1) == 0\n    if not valid:\n        diff = src.subs(f(x), hyp).doit()\n        diff = simplify(diff, seconds=1)\n        valid = diff == 0\n\n    return valid\n\n\ndef check_hypothesis(eq):\n    \"\"\"\n    Check a hypothesis for a given equation and its solution.\n    \"\"\"\n    env = Evaluator.ENV\n    src = idx_to_sp(env, eq['src'])\n    tgt = idx_to_sp(env, eq['tgt'])\n    hyp = eq['hyp']\n\n    hyp_infix = [env.id2word[wid] for wid in hyp]\n\n    try:\n        hyp, hyp_infix = idx_to_sp(env, hyp, return_infix=True)\n        is_valid = check_valid_solution(env, src, tgt, hyp)\n        if is_valid_expr(hyp_infix):\n            hyp_infix = str(hyp)\n\n    except (TimeoutError, Exception) as e:\n        e_name = type(e).__name__\n        if not isinstance(e, InvalidPrefixExpression):\n            logger.error(f\"Exception {e_name} when checking hypothesis: {hyp_infix}\")\n        hyp = f\"ERROR {e_name}\"\n        is_valid = False\n\n    # update hypothesis\n    f = env.local_dict['f']\n    x = env.local_dict['x']\n    eq['src'] = src.subs(f(x), 'f')  # hack to avoid pickling issues with lambdify\n    eq['tgt'] = tgt\n    eq['hyp'] = hyp_infix\n    eq['is_valid'] = is_valid\n\n    return eq\n\n\nclass Evaluator(object):\n\n    ENV = None\n\n    def __init__(self, trainer):\n        \"\"\"\n        Initialize evaluator.\n        \"\"\"\n        self.trainer = trainer\n        self.modules = trainer.modules\n        self.params = trainer.params\n        self.env = trainer.env\n        Evaluator.ENV = trainer.env\n\n    def run_all_evals(self):\n        \"\"\"\n        Run all evaluations.\n        \"\"\"\n        scores = OrderedDict({'epoch': self.trainer.epoch})\n\n        # save statistics about generated data\n        if self.params.export_data:\n            scores['total'] = sum(self.trainer.EQUATIONS.values())\n            scores['unique'] = len(self.trainer.EQUATIONS)\n            scores['unique_prop'] = 100. * scores['unique'] / scores['total']\n            return scores\n\n        with torch.no_grad():\n            for data_type in ['valid', 'test']:\n                for task in self.params.tasks:\n                    if self.params.beam_eval:\n                        self.enc_dec_step_beam(data_type, task, scores)\n                    else:\n                        self.enc_dec_step(data_type, task, scores)\n\n        return scores\n\n    def enc_dec_step(self, data_type, task, scores):\n        \"\"\"\n        Encoding / decoding step.\n        \"\"\"\n        params = self.params\n        env = self.env\n        encoder, decoder = self.modules['encoder'], self.modules['decoder']\n        encoder.eval()\n        decoder.eval()\n        assert params.eval_verbose in [0, 1]\n        assert params.eval_verbose_print is False or params.eval_verbose > 0\n        assert task in ['prim_fwd', 'prim_bwd', 'prim_ibp', 'ode1', 'ode2']\n\n        # stats\n        xe_loss = 0\n        n_valid = torch.zeros(1000, dtype=torch.long)\n        n_total = torch.zeros(1000, dtype=torch.long)\n\n        # evaluation details\n        if params.eval_verbose:\n            eval_path = os.path.join(params.dump_path, f\"eval.{task}.{scores['epoch']}\")\n            f_export = open(eval_path, 'w')\n            logger.info(f\"Writing evaluation results in {eval_path} ...\")\n\n        # iterator\n        iterator = self.env.create_test_iterator(data_type, task, params=params, data_path=self.trainer.data_path)\n        eval_size = len(iterator.dataset)\n\n        for (x1, len1), (x2, len2), nb_ops in iterator:\n\n            # print status\n            if n_total.sum().item() % 100 < params.batch_size:\n                logger.info(f\"{n_total.sum().item()}/{eval_size}\")\n\n            # target words to predict\n            alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n            pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n            y = x2[1:].masked_select(pred_mask[:-1])\n            assert len(y) == (len2 - 1).sum().item()\n\n            # cuda\n            x1, len1, x2, len2, y = to_cuda(x1, len1, x2, len2, y)\n\n            # forward / loss\n            encoded = encoder('fwd', x=x1, lengths=len1, causal=False)\n            decoded = decoder('fwd', x=x2, lengths=len2, causal=True, src_enc=encoded.transpose(0, 1), src_len=len1)\n            word_scores, loss = decoder('predict', tensor=decoded, pred_mask=pred_mask, y=y, get_scores=True)\n\n            # correct outputs per sequence / valid top-1 predictions\n            t = torch.zeros_like(pred_mask, device=y.device)\n            t[pred_mask] += word_scores.max(1)[1] == y\n            valid = (t.sum(0) == len2 - 1).cpu().long()\n\n            # export evaluation details\n            if params.eval_verbose:\n                for i in range(len(len1)):\n                    src = idx_to_sp(env, x1[1:len1[i] - 1, i].tolist())\n                    tgt = idx_to_sp(env, x2[1:len2[i] - 1, i].tolist())\n                    s = f\"Equation {n_total.sum().item() + i} ({'Valid' if valid[i] else 'Invalid'})\\nsrc={src}\\ntgt={tgt}\\n\"\n                    if params.eval_verbose_print:\n                        logger.info(s)\n                    f_export.write(s + \"\\n\")\n                    f_export.flush()\n\n            # stats\n            xe_loss += loss.item() * len(y)\n            n_valid.index_add_(-1, nb_ops, valid)\n            n_total.index_add_(-1, nb_ops, torch.ones_like(nb_ops))\n\n        # evaluation details\n        if params.eval_verbose:\n            f_export.close()\n\n        # log\n        _n_valid = n_valid.sum().item()\n        _n_total = n_total.sum().item()\n        logger.info(f\"{_n_valid}/{_n_total} ({100. * _n_valid / _n_total}%) equations were evaluated correctly.\")\n\n        # compute perplexity and prediction accuracy\n        assert _n_total == eval_size or self.trainer.data_path\n        scores[f'{data_type}_{task}_xe_loss'] = xe_loss / _n_total\n        scores[f'{data_type}_{task}_acc'] = 100. * _n_valid / _n_total\n\n        # per class perplexity and prediction accuracy\n        for i in range(len(n_total)):\n            if n_total[i].item() == 0:\n                continue\n            # logger.info(f\"{i}: {n_valid[i].item()} / {n_total[i].item()} ({100. * n_valid[i].item() / max(n_total[i].item(), 1)}%)\")\n            scores[f'{data_type}_{task}_acc_{i}'] = 100. * n_valid[i].item() / max(n_total[i].item(), 1)\n\n    def enc_dec_step_beam(self, data_type, task, scores):\n        \"\"\"\n        Encoding / decoding step with beam generation and SymPy check.\n        \"\"\"\n        params = self.params\n        env = self.env\n        encoder, decoder = self.modules['encoder'], self.modules['decoder']\n        encoder.eval()\n        decoder.eval()\n        assert params.eval_verbose in [0, 1, 2]\n        assert params.eval_verbose_print is False or params.eval_verbose > 0\n        assert task in ['prim_fwd', 'prim_bwd', 'prim_ibp', 'ode1', 'ode2']\n\n        # evaluation details\n        if params.eval_verbose:\n            eval_path = os.path.join(params.dump_path, f\"eval.{task}.{scores['epoch']}\")\n            f_export = open(eval_path, 'w')\n            logger.info(f\"Writing evaluation results in {eval_path} ...\")\n\n        def display_logs(logs, offset):\n            \"\"\"\n            Display detailed results about success / fails.\n            \"\"\"\n            if params.eval_verbose == 0:\n                return\n            for i, res in sorted(logs.items()):\n                n_valid = sum([int(v) for _, _, v in res['hyps']])\n                s = f\"Equation {offset + i} ({n_valid}/{len(res['hyps'])})\\nsrc={res['src']}\\ntgt={res['tgt']}\\n\"\n                for hyp, score, valid in res['hyps']:\n                    if score is None:\n                        s += f\"{int(valid)} {hyp}\\n\"\n                    else:\n                        s += f\"{int(valid)} {score :.3e} {hyp}\\n\"\n                if params.eval_verbose_print:\n                    logger.info(s)\n                f_export.write(s + \"\\n\")\n                f_export.flush()\n\n        # stats\n        xe_loss = 0\n        n_valid = torch.zeros(1000, params.beam_size, dtype=torch.long)\n        n_total = torch.zeros(1000, dtype=torch.long)\n\n        # iterator\n        iterator = env.create_test_iterator(data_type, task, params=params, data_path=self.trainer.data_path)\n        eval_size = len(iterator.dataset)\n\n        for (x1, len1), (x2, len2), nb_ops in iterator:\n\n            # target words to predict\n            alen = torch.arange(len2.max(), dtype=torch.long, device=len2.device)\n            pred_mask = alen[:, None] < len2[None] - 1  # do not predict anything given the last target word\n            y = x2[1:].masked_select(pred_mask[:-1])\n            assert len(y) == (len2 - 1).sum().item()\n\n            # cuda\n            x1, len1, x2, len2, y = to_cuda(x1, len1, x2, len2, y)\n            bs = len(len1)\n\n            # forward\n            encoded = encoder('fwd', x=x1, lengths=len1, causal=False)\n            decoded = decoder('fwd', x=x2, lengths=len2, causal=True, src_enc=encoded.transpose(0, 1), src_len=len1)\n            word_scores, loss = decoder('predict', tensor=decoded, pred_mask=pred_mask, y=y, get_scores=True)\n\n            # correct outputs per sequence / valid top-1 predictions\n            t = torch.zeros_like(pred_mask, device=y.device)\n            t[pred_mask] += word_scores.max(1)[1] == y\n            valid = (t.sum(0) == len2 - 1).cpu().long()\n\n            # save evaluation details\n            beam_log = {}\n            for i in range(len(len1)):\n                src = idx_to_sp(env, x1[1:len1[i] - 1, i].tolist())\n                tgt = idx_to_sp(env, x2[1:len2[i] - 1, i].tolist())\n                if valid[i]:\n                    beam_log[i] = {'src': src, 'tgt': tgt, 'hyps': [(tgt, None, True)]}\n\n            # stats\n            xe_loss += loss.item() * len(y)\n            n_valid[:, 0].index_add_(-1, nb_ops, valid)\n            n_total.index_add_(-1, nb_ops, torch.ones_like(nb_ops))\n\n            # continue if everything is correct. if eval_verbose, perform\n            # a full beam search, even on correct greedy generations\n            if valid.sum() == len(valid) and params.eval_verbose < 2:\n                display_logs(beam_log, offset=n_total.sum().item() - bs)\n                continue\n\n            # invalid top-1 predictions - check if there is a solution in the beam\n            invalid_idx = (1 - valid).nonzero().view(-1)\n            logger.info(f\"({n_total.sum().item()}/{eval_size}) Found {bs - len(invalid_idx)}/{bs} valid top-1 predictions. Generating solutions ...\")\n\n            # generate\n            _, _, generations = decoder.generate_beam(\n                encoded.transpose(0, 1),\n                len1,\n                beam_size=params.beam_size,\n                length_penalty=params.beam_length_penalty,\n                early_stopping=params.beam_early_stopping,\n                max_len=params.max_len\n            )\n\n            # prepare inputs / hypotheses to check\n            # if eval_verbose < 2, no beam search on equations solved greedily\n            inputs = []\n            for i in range(len(generations)):\n                if valid[i] and params.eval_verbose < 2:\n                    continue\n                for j, (score, hyp) in enumerate(sorted(generations[i].hyp, key=lambda x: x[0], reverse=True)):\n                    inputs.append({\n                        'i': i,\n                        'j': j,\n                        'score': score,\n                        'src': x1[1:len1[i] - 1, i].tolist(),\n                        'tgt': x2[1:len2[i] - 1, i].tolist(),\n                        'hyp': hyp[1:].tolist(),\n                    })\n\n            # check hypotheses with multiprocessing\n            outputs = []\n            with ProcessPoolExecutor(max_workers=20) as executor:\n                for output in executor.map(check_hypothesis, inputs, chunksize=1):\n                    outputs.append(output)\n\n            # read results\n            for i in range(bs):\n\n                # select hypotheses associated to current equation\n                gens = sorted([o for o in outputs if o['i'] == i], key=lambda x: x['j'])\n                assert (len(gens) == 0) == (valid[i] and params.eval_verbose < 2) and (i in beam_log) == valid[i]\n                if len(gens) == 0:\n                    continue\n\n                # source / target\n                src = gens[0]['src']\n                tgt = gens[0]['tgt']\n                beam_log[i] = {'src': src, 'tgt': tgt, 'hyps': []}\n\n                # for each hypothesis\n                for j, gen in enumerate(gens):\n\n                    # sanity check\n                    assert gen['src'] == src and gen['tgt'] == tgt and gen['i'] == i and gen['j'] == j\n\n                    # if the hypothesis is correct, and we did not find a correct one before\n                    is_valid = gen['is_valid']\n                    if is_valid and not valid[i]:\n                        n_valid[nb_ops[i], j] += 1\n                        valid[i] = 1\n\n                    # update beam log\n                    beam_log[i]['hyps'].append((gen['hyp'], gen['score'], is_valid))\n\n            # valid solutions found with beam search\n            logger.info(f\"    Found {valid.sum().item()}/{bs} solutions in beam hypotheses.\")\n\n            # export evaluation details\n            if params.eval_verbose:\n                assert len(beam_log) == bs\n                display_logs(beam_log, offset=n_total.sum().item() - bs)\n\n        # evaluation details\n        if params.eval_verbose:\n            f_export.close()\n            logger.info(f\"Evaluation results written in {eval_path}\")\n\n        # log\n        _n_valid = n_valid.sum().item()\n        _n_total = n_total.sum().item()\n        logger.info(f\"{_n_valid}/{_n_total} ({100. * _n_valid / _n_total}%) equations were evaluated correctly.\")\n        logger.info(n_valid[\n            :(n_valid.sum(1) > 0).nonzero().view(-1)[-1] + 1,\n            :(n_valid.sum(0) > 0).nonzero().view(-1)[-1] + 1\n        ])\n\n        # compute perplexity and prediction accuracy\n        assert _n_total == eval_size or self.trainer.data_path\n        scores[f'{data_type}_{task}_beam_acc'] = 100. * _n_valid / _n_total\n\n        # per class perplexity and prediction accuracy\n        for i in range(len(n_total)):\n            if n_total[i].item() == 0:\n                continue\n            logger.info(f\"{i}: {n_valid[i].sum().item()} / {n_total[i].item()} ({100. * n_valid[i].sum().item() / max(n_total[i].item(), 1)}%)\")\n            scores[f'{data_type}_{task}_beam_acc_{i}'] = 100. * n_valid[i].sum().item() / max(n_total[i].item(), 1)\n\n\ndef convert_to_text(batch, lengths, id2word, params):\n    \"\"\"\n    Convert a batch of sequences to a list of text sequences.\n    \"\"\"\n    batch = batch.cpu().numpy()\n    lengths = lengths.cpu().numpy()\n\n    slen, bs = batch.shape\n    assert lengths.max() == slen and lengths.shape[0] == bs\n    assert (batch[0] == params.eos_index).sum() == bs\n    assert (batch == params.eos_index).sum() == 2 * bs\n    sequences = []\n\n    for j in range(bs):\n        words = []\n        for k in range(1, lengths[j]):\n            if batch[k, j] == params.eos_index:\n                break\n            words.append(id2word[batch[k, j]])\n        sequences.append(\" \".join(words))\n    return sequences\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/evaluator.py b/src/evaluator.py
--- a/src/evaluator.py	(revision 7e7b6f89b7e188621bb4e01ef8fa84b57d0b0877)
+++ b/src/evaluator.py	(date 1654612084819)
@@ -56,12 +56,15 @@
     Check a hypothesis for a given equation and its solution.
     """
     env = Evaluator.ENV
+    #print(f'env: {env}')
     src = idx_to_sp(env, eq['src'])
     tgt = idx_to_sp(env, eq['tgt'])
+    #print(f'src: {src}')
+    #print(f'tgt: {tgt}')
     hyp = eq['hyp']
-
+    #print(f'hip: {hip}')
     hyp_infix = [env.id2word[wid] for wid in hyp]
-
+    #print(f'hyp_infix: {hyp_infix}')
     try:
         hyp, hyp_infix = idx_to_sp(env, hyp, return_infix=True)
         is_valid = check_valid_solution(env, src, tgt, hyp)
@@ -94,6 +97,7 @@
         """
         Initialize evaluator.
         """
+        print('initislizing evaluator')
         self.trainer = trainer
         self.modules = trainer.modules
         self.params = trainer.params
@@ -127,6 +131,7 @@
         """
         Encoding / decoding step.
         """
+        print('------------------------ enc_dec_step')
         params = self.params
         env = self.env
         encoder, decoder = self.modules['encoder'], self.modules['decoder']
@@ -164,13 +169,18 @@
             assert len(y) == (len2 - 1).sum().item()
 
             # cuda
+            #print(f'x1 {x1}, x2 {x2}, len1 {len1}, len2 {len2}, y {y}')
             x1, len1, x2, len2, y = to_cuda(x1, len1, x2, len2, y)
-
+            #print(f'x1 {x1}, x2 {x2}, len1 {len1}, len2 {len2}, y {y}')
             # forward / loss
             encoded = encoder('fwd', x=x1, lengths=len1, causal=False)
             decoded = decoder('fwd', x=x2, lengths=len2, causal=True, src_enc=encoded.transpose(0, 1), src_len=len1)
             word_scores, loss = decoder('predict', tensor=decoded, pred_mask=pred_mask, y=y, get_scores=True)
 
+            #print(f'encoded {encoded}')
+            #print(f'decoded {decoded}')
+            #print(f'word_scores {word_scores}')
+
             # correct outputs per sequence / valid top-1 predictions
             t = torch.zeros_like(pred_mask, device=y.device)
             t[pred_mask] += word_scores.max(1)[1] == y
@@ -217,6 +227,7 @@
         """
         Encoding / decoding step with beam generation and SymPy check.
         """
+        print('------------------------ enc_dec_step_BEAM')
         params = self.params
         env = self.env
         encoder, decoder = self.modules['encoder'], self.modules['decoder']
Index: evaluation.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/bin/bash\n\nPARAMS=(\n  ## main parameters\n  --exp_name first_eval     # experiment name\n  --eval_only true          # evaluation mode (do not load the training set)\n  --reload_model \"fwd.pth\"  # model to reload and evaluate\n\n  ## dataset location\n  --tasks \"prim_fwd\"                                                    # task\n  --reload_data \"prim_fwd,prim_fwd.train,prim_fwd.valid,prim_fwd.test\"  # data location\n\n  --emb_dim 1024    # model dimension\n  --n_enc_layers 6  # encoder layers\n  --n_dec_layers 6  # decoder layers\n  --n_heads 8       # number of heads\n\n  ## evaluation parameters\n  --beam_eval true            # beam evaluation (with false, outputs are only compared with dataset solutions)\n  --beam_size 10              # beam size\n  --beam_length_penalty 1.0   # beam length penalty (1.0 corresponds to average of log-probs)\n  --beam_early_stopping 1     # beam early stopping\n  --eval_verbose 1            # export beam results (set to 2 to evaluate with beam even when greedy was successful)\n  --eval_verbose_print false  # print detailed evaluation results\n\n)\n\npython3 main.py ${PARAMS[@]}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/evaluation.sh b/evaluation.sh
--- a/evaluation.sh	(revision 7e7b6f89b7e188621bb4e01ef8fa84b57d0b0877)
+++ b/evaluation.sh	(date 1654023052764)
@@ -2,13 +2,15 @@
 
 PARAMS=(
   ## main parameters
-  --exp_name first_eval     # experiment name
+  --exp_name exp     # experiment name
+  --cpu true
   --eval_only true          # evaluation mode (do not load the training set)
-  --reload_model "fwd.pth"  # model to reload and evaluate
+  --reload_model "dumped/fwd.pth"  # model to reload and evaluate
+
 
   ## dataset location
   --tasks "prim_fwd"                                                    # task
-  --reload_data "prim_fwd,prim_fwd.train,prim_fwd.valid,prim_fwd.test"  # data location
+  --reload_data "prim_fwd,dumped/prim_fwd.train,dumped/prim_fwd.valid,dumped/prim_fwd.test"  # data location
 
   --emb_dim 1024    # model dimension
   --n_enc_layers 6  # encoder layers
@@ -21,7 +23,7 @@
   --beam_length_penalty 1.0   # beam length penalty (1.0 corresponds to average of log-probs)
   --beam_early_stopping 1     # beam early stopping
   --eval_verbose 1            # export beam results (set to 2 to evaluate with beam even when greedy was successful)
-  --eval_verbose_print false  # print detailed evaluation results
+  --eval_verbose_print true  # print detailed evaluation results
 
 )
 
